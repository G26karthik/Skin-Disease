# Supplementary PBL Report — Skin Disease AI Decision Support

This supplementary report has been updated to follow the structure and ordering of `SUMMARY_PBL.md`. It consolidates the project's key artifacts, final metrics, per-class results, reproduction steps, and next steps for academic review. All referenced images are in `docs/images/` and were generated by the project's reporting scripts.

---

## Executive Summary

The project trains an EfficientNet_B0-based classifier on the HAM10000 dermatoscopic dataset to classify 7 lesion types. The final full training run (30 epochs using combined train+validation splits) yields a test accuracy of 85.77% and macro F1 of 75.12%. The system includes Grad‑CAM interpretability, calibration analyses (ECE, Brier), and inference profiling (latency/throughput). This supplementary report lists the visual artifacts, numeric results, reproduction instructions, and suggested next steps in the same ordering as the main PBL summary.

Key outcomes (full training):
- Test Accuracy: 0.8577
- Macro F1: 0.7512
- ECE: 0.0450
- Mean latency: 6.29 ms

---

## Visual Artifacts (locations)

The following artifacts are produced by `scripts/generate_report_figures.py` from the saved run and model artifacts in `runs/` and `models/`:

- Architecture diagram: `docs/images/architecture.png`
- Class distribution: `docs/images/class_distribution.png`
- Grad-CAM montage (examples): `docs/images/gradcam_montage.png`
- Confusion matrix (normalized): `docs/images/confusion_matrix.png`
- Reliability (calibration) diagram: `docs/images/reliability.png`
- Training curves: `docs/images/training_curves.png`
- Per-class metrics plot: `docs/images/per_class_metrics.png`
- Latency profile: `docs/images/latency.png`
- Roadmap / project timeline: `docs/images/roadmap.png`

### Sample Test Images (one per class)

Representative, real dermatoscopic samples from the HAM10000 test set are included for manual inspection and illustrative Grad-CAM overlays (copied into the repo despite `.gitignore` rules):

- Actinic keratoses: `docs/images/samples/actinic_keratoses.jpg`
- Basal cell carcinoma: `docs/images/samples/basal_cell_carcinoma.jpg`
- Benign keratosis: `docs/images/samples/benign_keratosis.jpg`
- Dermatofibroma: `docs/images/samples/dermatofibroma.jpg`
- Melanoma: `docs/images/samples/melanoma.jpg`
- Melanocytic nevi: `docs/images/samples/melanocytic_nevi.jpg`
- Vascular lesions: `docs/images/samples/vascular_lesions.jpg`

Note: These images are included for demonstrative and educational use only and are not to be used for clinical decision-making.

---

## Final Quantitative Results (from `runs/test_eval_metrics.json`)

Summary metrics (test set):

- Test Accuracy: 0.8577154308617234 (~85.77%)
- Macro Precision: 0.7710219076669106
- Macro Recall: 0.7395027738947345
- Macro F1: 0.7512110404245183 (~75.12%)
- Weighted F1: 0.8555148080462415
- Expected Calibration Error (ECE): 0.0450
- Brier Score: 0.2764
- Mean Inference Latency: 0.00629 s (~6.29 ms)
- Throughput: 158.89 images/s

Per-class breakdown (precision / recall / f1 / support):

- Actinic_keratoses: precision 0.6667, recall 0.62745, f1 0.64646, support 102
- Basal_cell_carcinoma: precision 0.75342, recall 0.71429, f1 0.73333, support 154
- Benign_keratosis: precision 0.74667, recall 0.71338, f1 0.72964, support 314
- Dermatofibroma: precision 0.60714, recall 0.77273, f1 0.68000, support 44
- Melanoma: precision 0.70667, recall 0.63095, f1 0.66667, support 336
- Melanocytic_nevi: precision 0.91659, recall 0.94500, f1 0.93058, support 2000
- Vascular_lesions: precision 1.00000, recall 0.77273, f1 0.87179, support 44

These values are pulled directly from `runs/test_eval_metrics.json` and are used across the summary and figures.

---

## How to Reproduce Key Artifacts (short commands)

Train (example):

```powershell
python -m src.train --data_dir data --epochs 30 --batch_size 32
```

Generate report-quality figures (after training / evaluation):

```powershell
python scripts/generate_report_figures.py
```

Run the Streamlit demo (ensure `models/model.pt` exists):

```powershell
streamlit run app.py
```

Notes:
- The reporting script reads `runs/test_eval_metrics.json`, `runs/history.json` and `models/confusion_matrix.npy` to create the figures under `docs/images/`.
- If figures are stale, re-run `scripts/generate_report_figures.py` after confirming `runs/` and `models/` contain the expected artifacts.

---

## Observations and Next Steps (aligned with Summary)

- Calibration: run temperature scaling (using validation logits) to attempt to reduce ECE below the target (≈0.035). A planned script is recommended.
- Minority-class improvements: apply class-balanced loss or focal loss, stronger augmentation, and include external datasets to improve melanoma recall (target ≥0.85 for triage use-case).
- Explainability: expand from Grad‑CAM to SHAP/LIME and counterfactual analysis for auditability.
- External validation and fairness testing: evaluate across datasets with broader skin tone diversity to quantify bias and generalization.

---

## Artifacts Location (quick reference)

- Model weights and checkpoints: `models/`
- Confusion matrix: `models/confusion_matrix.npy`
- Run metrics and history: `runs/` (including `test_eval_metrics.json`, `history.json`, `last_run.json`)
- Figures and report images: `docs/images/`
- Sample test images used in this supplementary report: `docs/images/samples/`

---

## Bibliography / References

1. Tschandl P. et al. HAM10000 Dataset. *Scientific Data* (2018).
2. Tan M., Le Q. EfficientNet: Rethinking Model Scaling for CNNs. *ICML* (2019).
3. Selvaraju R.R. et al. Grad-CAM: Visual Explanations from Deep Networks. *ICCV* (2017).
4. Guo C. et al. On Calibration of Modern Neural Networks. *ICML* (2017).
5. He K. et al. Deep Residual Learning for Image Recognition. *CVPR* (2016).

---

*This supplementary report was updated to reflect the full-training artifacts and ordered to match the PBL summary on October 29, 2025.*
